env_name: HumanoidRemainStanding # RL environment name
base_path: ./data/runs/training # Base path to save logs and checkpoints
seed: 42 # Seed for reproducibility
description: Humanoid training using CLIP reward
tags: # Wandb tags
  - training
  - humanoid
  - CLIP
reward:
  name: ground_truth
rl:
  policy_name: MlpPolicy
  n_steps: 10000000 # Total number of simulation steps to be collected. [From paper: 10000000]
  n_envs_per_worker: 8 # Number of environments per worker (GPU)
  episode_length: 100 # Desired episode length [From paper: 100, from this other paper that actually does humanoid standup (), they did 240]
  learning_starts: 10000 # Number of env steps to collect before training [From paper: 50000]
  train_freq: 100 # Number of collected env steps between training iterations [From paper: 100, assume this should match episode length]
  batch_size: 64 # SAC buffer sample size per gradient step
  gradient_steps: 1 # Number of samples to collect from the buffer per training step
  tau: 0.005 # SAC target network update rate [From paper: 0.005]
  gamma: 0.95 # SAC discount factor [From paper: 0.95]
  learning_rate: 6e-4 # SAC optimizer learning rate [From paper: 6e-4]
logging:
  checkpoint_freq: 240000 # Number of env steps between checkpoints [From paper: 128000]
  video_freq: 120000 # Number of env steps between videos
  tensorboard_freq: 120000
# rl:
#   policy_name: MlpPolicy
#   n_steps: 2000000 # Total number of simulation steps to be collected. [From paper: 10000000]
#   n_envs_per_worker: 8 # Number of environments per worker (GPU)
#   episode_length: 240 # Desired episode length [From paper: 100, from this other paper that actually does humanoid standup (), they did 240]
#   learning_starts: 10000 # Number of env steps to collect before training [From paper: 50000]
#   train_freq: 240 # Number of collected env steps between training iterations [From paper: 100, assume this should match episode length]
#   batch_size: 64 # SAC buffer sample size per gradient step
#   gradient_steps: 1 # Number of samples to collect from the buffer per training step
#   tau: 0.005 # SAC target network update rate [From paper: 0.005]
#   gamma: 0.95 # SAC discount factor [From paper: 0.95]
#   learning_rate: 6e-4 # SAC optimizer learning rate [From paper: 6e-4]
# logging:
#   checkpoint_freq: 76800 # Number of env steps between checkpoints [From paper: 128000]
#   video_freq: 25600 # Number of env steps between videos
#   tensorboard_freq: 25600
# rl:
#   policy_name: MlpPolicy
#   n_steps: 20000 # Total number of simulation steps to be collected. [From paper: 10000000]
#   n_envs_per_worker: 8 # Number of environments per worker (GPU)
#   episode_length: 100 # Desired episode length [From paper: 100, from this other paper that actually does humanoid standup (), they did 240]
#   learning_starts: 1000 # Number of env steps to collect before training [From paper: 50000]
#   train_freq: 240 # Number of collected env steps between training iterations [From paper: 100, assume this should match episode length]
#   batch_size: 64 # SAC buffer sample size per gradient step
#   gradient_steps: 1 # Number of samples to collect from the buffer per training step
#   tau: 0.005 # SAC target network update rate [From paper: 0.005]
#   gamma: 0.95 # SAC discount factor [From paper: 0.95]
#   learning_rate: 6e-4 # SAC optimizer learning rate [From paper: 6e-4]
# logging:
#   checkpoint_freq: 2400 # Number of env steps between checkpoints [From paper: 128000]
#   video_freq: 2400 # Number of env steps between videos
#   tensorboard_freq: 2400